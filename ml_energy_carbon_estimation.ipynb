{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install transformers codecarbon datasets trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from codecarbon import EmissionsTracker\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:58:47.313466Z","iopub.execute_input":"2025-02-02T15:58:47.313841Z","iopub.status.idle":"2025-02-02T15:58:47.317863Z","shell.execute_reply.started":"2025-02-02T15:58:47.313817Z","shell.execute_reply":"2025-02-02T15:58:47.316939Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:58:47.467907Z","iopub.execute_input":"2025-02-02T15:58:47.468259Z","iopub.status.idle":"2025-02-02T15:58:50.312434Z","shell.execute_reply.started":"2025-02-02T15:58:47.468227Z","shell.execute_reply":"2025-02-02T15:58:50.311721Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"tracker = EmissionsTracker(output_file=\"./test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:58:50.313543Z","iopub.execute_input":"2025-02-02T15:58:50.313873Z","iopub.status.idle":"2025-02-02T15:58:54.624975Z","shell.execute_reply.started":"2025-02-02T15:58:50.313845Z","shell.execute_reply":"2025-02-02T15:58:54.624281Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 15:58:50] [setup] RAM Tracking...\n[codecarbon INFO @ 15:58:50] [setup] CPU Tracking...\n[codecarbon WARNING @ 15:58:50] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 15:58:51] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 15:58:51] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 15:58:51] [setup] GPU Tracking...\n[codecarbon INFO @ 15:58:51] Tracking Nvidia GPU via pynvml\n[codecarbon INFO @ 15:58:51] >>> Tracker's metadata:\n[codecarbon INFO @ 15:58:51]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 15:58:51]   Python version: 3.10.12\n[codecarbon INFO @ 15:58:51]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 15:58:51]   Available RAM : 31.351 GB\n[codecarbon INFO @ 15:58:51]   CPU count: 4\n[codecarbon INFO @ 15:58:51]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 15:58:51]   GPU count: 2\n[codecarbon INFO @ 15:58:51]   GPU model: 2 x Tesla T4\n[codecarbon INFO @ 15:58:54] Saving emissions data to file /kaggle/working/test.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"user_message = \"Don't forget to like and subscribe!\"\ninputs = tokenizer(user_message, return_tensors=\"pt\")\n\ntracker.start()\noutputs = model.generate(**inputs.to(\"cuda\"),\n                         max_new_tokens=12,\n                         num_beams=10,\n                         early_stopping=True,\n                         no_repeat_ngram_size=2)\ntracker.stop()\n\nresult = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(result[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:58:54.626409Z","iopub.execute_input":"2025-02-02T15:58:54.626674Z","iopub.status.idle":"2025-02-02T15:58:56.294832Z","shell.execute_reply.started":"2025-02-02T15:58:54.626652Z","shell.execute_reply":"2025-02-02T15:58:56.293951Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 15:58:56] Energy consumed for RAM : 0.000005 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 15:58:56] Energy consumed for all CPUs : 0.000019 kWh. Total CPU Power : 42.5 W\n[codecarbon INFO @ 15:58:56] Energy consumed for all GPUs : 0.000037 kWh. Total GPU Power : 81.30649897836354 W\n[codecarbon INFO @ 15:58:56] 0.000062 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Don't forget to like and subscribe! https://www.youtube.com/watch?v=\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTTrainer, SFTConfig\nimport os\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ndataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1%]\").select(range(20))\n\ntraining_params = SFTConfig(\n    output_dir=\"./model_train\",\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    dataset_text_field=\"text\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    args=training_params,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:54:59.504232Z","iopub.execute_input":"2025-02-02T15:54:59.504565Z","iopub.status.idle":"2025-02-02T15:55:41.468288Z","shell.execute_reply.started":"2025-02-02T15:54:59.504541Z","shell.execute_reply":"2025-02-02T15:55:41.467250Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feb5b04089d3474aa96e4bfa1f782252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(â€¦)-00000-of-00001-a09b74b3ef9c3b56.parquet:   0%|          | 0.00/24.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9632948356b04f288568eda959e2fab7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04056814450248879035af1321bf3d86"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n<ipython-input-1-af90cc280fc9>:22: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  trainer = SFTTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5addccd7efc946fdb4febe9964707675"}},"metadata":{}},{"name":"stderr","text":"[codecarbon INFO @ 15:55:15] [setup] RAM Tracking...\n[codecarbon INFO @ 15:55:15] [setup] CPU Tracking...\n[codecarbon WARNING @ 15:55:15] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 15:55:16] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 15:55:16] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 15:55:16] [setup] GPU Tracking...\n[codecarbon INFO @ 15:55:16] Tracking Nvidia GPU via pynvml\n[codecarbon INFO @ 15:55:16] >>> Tracker's metadata:\n[codecarbon INFO @ 15:55:16]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 15:55:16]   Python version: 3.10.12\n[codecarbon INFO @ 15:55:16]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 15:55:16]   Available RAM : 31.351 GB\n[codecarbon INFO @ 15:55:16]   CPU count: 4\n[codecarbon INFO @ 15:55:16]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 15:55:16]   GPU count: 2\n[codecarbon INFO @ 15:55:16]   GPU model: 2 x Tesla T4\n[codecarbon INFO @ 15:55:19] Saving emissions data to file /kaggle/working/model_train/emissions.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 00:20, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"[codecarbon INFO @ 15:55:34] Energy consumed for RAM : 0.000049 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 15:55:34] Energy consumed for all CPUs : 0.000178 kWh. Total CPU Power : 42.5 W\n[codecarbon INFO @ 15:55:34] Energy consumed for all GPUs : 0.000401 kWh. Total GPU Power : 95.59812257181933 W\n[codecarbon INFO @ 15:55:34] 0.000629 kWh of electricity used since the beginning.\n[codecarbon INFO @ 15:55:41] Energy consumed for RAM : 0.000071 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 15:55:41] Energy consumed for all CPUs : 0.000255 kWh. Total CPU Power : 42.5 W\n[codecarbon INFO @ 15:55:41] Energy consumed for all GPUs : 0.000511 kWh. Total GPU Power : 60.56832324753425 W\n[codecarbon INFO @ 15:55:41] 0.000837 kWh of electricity used since the beginning.\n/usr/local/lib/python3.10/dist-packages/codecarbon/output_methods/file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20, training_loss=1.1072583198547363, metrics={'train_runtime': 21.6968, 'train_samples_per_second': 0.922, 'train_steps_per_second': 0.922, 'total_flos': 15101811781632.0, 'train_loss': 1.1072583198547363, 'epoch': 1.0})"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}